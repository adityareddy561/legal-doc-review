# ğŸ“ Legal Document Review RAG App

This project is an AI-powered legal document review tool using Retrieval-Augmented Generation (RAG).  
It lets users:
âœ… Upload PDF legal documents (contracts, NDAs, agreements)  
âœ… Extract, chunk, and embed the text  
âœ… Store embeddings in a PostgreSQL vector database with `pgvector`  
âœ… Generate a smart context-aware summary  
âœ… Ask follow-up questions safely using semantic search and your LLM  
âœ… Keep context isolated using session state

---

## ğŸ“‚ **Project Structure**

```
legal-doc-review/
 â”œâ”€â”€ app.py              # FastAPI app with upload/query routes
 â”œâ”€â”€ db.py               # DB init script (pgvector & legal_chunks table)
 â”œâ”€â”€ templates/index.html # Frontend (Jinja2Templates)
 â”œâ”€â”€ static/style.css    # Stylesheet
 â”œâ”€â”€ .env                # Your environment variables
 â”œâ”€â”€ requirements.txt    # Python dependencies
```

---

## âš™ï¸ **Setup**

### âœ… 1. Clone the repo

```bash
git clone <your-repo-url>
cd legal-doc-review
```

---

### âœ… 2. Create & activate a virtual environment

```bash
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

---

### âœ… 3. Install dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ”‘ **4. Create `.env`**

```env
OPENAI_API_KEY=<your-openai-key>
POSTGRES_CONNECTION=postgresql+psycopg://langchain:langchain@localhost:6024/langchain
```

---

## ğŸ˜ **5. Start PostgreSQL + pgvector**

Run Postgres vector DB in Docker:

```bash
docker run --name pgvector-container   -e POSTGRES_USER=langchain   -e POSTGRES_PASSWORD=langchain   -e POSTGRES_DB=langchain   -p 6024:5432   -d pgvector/pgvector:pg16
```

Or use a `docker-compose.yml` if you prefer.

---

## ğŸ—„ï¸ **6. Initialize the DB**

Run:

```bash
python db.py
```

This will:

- Create the `vector` extension (`pgvector`)
- Create the `legal_chunks` table
- Add a composite index for fast retrieval

---

## âš¡ **7. Run the app**

Start your FastAPI server:

```bash
uvicorn app:app --reload
```

Open [http://127.0.0.1:8000](http://127.0.0.1:8000)

---

## ğŸ’» **How it works**

âœ… **Upload**

- Upload a PDF â†’ text is extracted, chunked, embedded with OpenAI embeddings.
- Chunks are stored in `pgvector` with `document_id` and `user_id` metadata.
- Server generates a summary using your LLM.
- `document_id` is stored in session so follow-ups use the right context.

âœ… **Ask**

- You can ask multiple questions about the same doc.
- The vector store does semantic similarity search, filtering by `document_id` & `user_id`.
- The LLM only uses the retrieved chunks â€” if thereâ€™s no info, it says _â€œI donâ€™t know.â€_

âœ… **Frontend**

- Simple upload & query form (`index.html`).
- Styled with `static/style.css`.
- Uses `SessionMiddleware` to keep context across requests.

---

## ğŸ” **Session State**

Session cookies store your `document_id` & `user_id` so your queries never mix different uploads.

---

## ğŸ§  **Agentic AI vs. Basic RAG**

This project is classic RAG:

- Semantic search + rewriting.
- No tool-calling, fallback loops, or planning.

ğŸ‘‰ For **Agentic AI**, youâ€™d extend this with:

- Multi-step plans (e.g., fallback to regex if similarity fails)
- Conditional tool use (e.g., keyword search)
- Clarification loops with the user

---

## âœ… **Next Steps**

- [ ] Add fallback keyword/regex match for exact clause lookups.
- [ ] Add metadata: store `clause_number` for fast lookups.
- [ ] Add multi-user auth instead of `demo-user`.
- [ ] Add streaming response for large answers.
- [ ] Deploy to Railway, Render, or your favorite cloud.

---

## ğŸ¤ **License**

MIT â€” use freely for learning and demos!

---

## ğŸ“¬ **Questions?**

Open an issue or reach out!

---

**Built with â¤ï¸ using FastAPI, LangChain, pgvector, and OpenAI.**
